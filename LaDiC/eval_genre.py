import sys
import json
from tqdm import tqdm
import os
import torch
from PIL import Image
import argparse
import scipy.io.wavfile as wavfile
from pydub import AudioSegment
from transformers import AutoProcessor, GPT2Tokenizer, pipeline, set_seed, BertTokenizer, MusicgenForConditionalGeneration
# from models.music_captioning_ladic import MusicCaptioningLaDiC
from diffusers import AudioLDM2Pipeline
import numpy as np
import soundfile as sf
from diff_models.diffusion import *
from torch import nn
from diff_models.ladic_lora import Diffuser, Diffuser_with_LN
from my_utils.blip_util import load_checkpoint
from safetensors.torch import load_file # safetensors import ì¶”ê°€
import time
device = torch.device('cuda')
from torchvision.datasets.utils import download_url
# from evaluate import load
torch.backends.cudnn.benchmark = False
from io import BytesIO
from loguru import logger
import scipy.io.wavfile
import re # ìž¥ë¥´ íŒŒì‹±ì„ ìœ„í•´ re import
from collections import Counter # ìž¥ë¥´ ì¹´ìš´íŒ…ì„ ìœ„í•´ Counter import

# --- 1. ìž¥ë¥´ ì˜¨í†¨ë¡œì§€ ë° í‚¤ì›Œë“œ ì •ì˜ (ì‹ ê·œ ì¶”ê°€) ---
# 'Parent_Genre': ['keyword1', 'keyword2', ...]
# (ì¤‘ìš”) 'rock and roll'ì²˜ëŸ¼ ê¸´ í‚¤ì›Œë“œë¥¼ 'rock'ë³´ë‹¤ ë¨¼ì € ë°°ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
GENRE_ONTOLOGY = {
    'Rock': [
        'rock and roll', 'blues-rock', 'hard rock', 'folk rock', 'pop rock', 
        'metal', 'heavy metal', 'rock song', 'rock', 'punk', 'grungy'
    ],
    'Blues': ['blues', 'bluesy', 'country blues'],
    'Jazz': ['jazz', 'jazz fusion', 'swing', 'big band', 'jazzy', 'bossa nova'],
    'Classical': [
        'classical', 'orchestral', 'symphony', 'baroque', 'classical piece',
        'strings', 'string section', 'violin', 'cello', 'viola', 'harp',
        'harpsichord', 'piano solo' # í”¼ì•„ë…¸/í˜„ì•…ê¸° ê´€ë ¨ í‚¤ì›Œë“œ ë³´ê°•
    ],
    'Electronic': [
        'electronic', 'techno', 'trance', 'ambient', 'new age', 'synth',
        'synthesizer', 'edm', 'house', 'industrial', 'electro', 
        'electronic dance music'
    ],
    'Folk/Country': [
        'folk', 'country', 'bluegrass', 'folk song', 'acoustic', 'banjo', 
        'mandolin', 'ukulele', 'fiddle'
    ],
    'Pop': ['pop', 'pop song', 'synth-pop', 'k-pop'],
    'Funk/Soul': ['funk', 'funky', 'soul', 'soulful', 'groovy', 'r&b', 'disco'],
    'Hip Hop': ['hip hop', 'rap'],
    'Latin': ['latin', 'salsa', 'latin dance'],
    'World': [
        'indian classical', 'indian', 'sitar', 'tabla', 'didgeridoo', 
        'bagpipes', 'traditional folk', 'aboriginal', 'arabic'
    ],
    # 'Other' ì¹´í…Œê³ ë¦¬ëŠ” í‰ê°€ì—ì„œ ì œì™¸í•˜ë¯€ë¡œ, í‚¤ì›Œë“œê°€ ì ì–´ë„ ë¨
    'Other': ['lullaby', 'jig', 'experimental', 'fusion', 'ambient', 'drone', 'instrumental'] 
}

def build_keyword_map():
    """ { 'keyword': 'Parent_Genre' } ë§µê³¼ ì •ë ¬ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„± """
    parent_map = {}
    all_keywords = []
    
    for parent, children in GENRE_ONTOLOGY.items():
        for child in children:
            parent_map[child] = parent
            all_keywords.append(child)
            
    # ê¸´ í‚¤ì›Œë“œê°€ ë¨¼ì € ë§¤ì¹˜ë˜ë„ë¡ ì •ë ¬ (ì˜ˆ: "folk rock" > "rock")
    sorted_keywords = sorted(all_keywords, key=len, reverse=True)
    return parent_map, sorted_keywords

def get_parent_genre(text: str, keywords_list: list, parent_map: dict) -> str:
    """ í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ë¡œ ë°œê²¬ë˜ëŠ” ìž¥ë¥´ í‚¤ì›Œë“œì˜ ìƒìœ„ ìž¥ë¥´ë¥¼ ë°˜í™˜ """
    if not text:
        return None
        
    text_low = text.lower()
    for keyword in keywords_list:
        # \b: ë‹¨ì–´ ê²½ê³„ë¥¼ í™•ì¸í•˜ì—¬ 'rock'ì´ 'rocket'ì— ë§¤ì¹˜ë˜ëŠ” ê²ƒì„ ë°©ì§€
        if re.search(r'\b' + re.escape(keyword) + r'\b', text_low):
            return parent_map[keyword]
    return None # ë§¤ì¹˜ë˜ëŠ” ìž¥ë¥´ ì—†ìŒ


# --- 2. ê¸°ì¡´ ì½”ë“œ (ì¸ìˆ˜ ìˆ˜ì •) ---

# Argument parsing
parser = argparse.ArgumentParser()
parser.add_argument(
    '--output_dir', default="/mnt/storage1/Jin/diffMusic/result/test31_muimage_best2", type=str,
    help='Directory to save the evaluation result JSON'
)
parser.add_argument(
    "--clip_model", default="openai/clip-vit-base-patch32", type=str,
    help="Path or name of the CLIP model"
)
# music_decoder ì¸ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ
# parser.add_argument(
# Â  Â  "--music_decoder", default="audioldm", type=str, choices=["musicgen", "audioldm"],
# Â  Â  help="Music generation model to use: musicgen or audioldm"
# )

args = parser.parse_args()

# Load the LaDiC model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 3. ê¸°ì¡´ ì½”ë“œ (ìˆ˜ì • ì—†ìŒ) ---
# (inference, load_model, generate_music_caption, generate_music, model_evaluate í•¨ìˆ˜)
# (generate_music_captionì™€ generate_musicëŠ” mainì—ì„œ í˜¸ì¶œë˜ì§€ ì•Šì§€ë§Œ, 
#  í˜¹ì‹œ ëª¨ë¥¼ ì˜ì¡´ì„±ì„ ìœ„í•´ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.)

def inference(x, tokenizer, model, time_difference = 0):
    x_t = torch.randn((x["image"].shape[0], MAX_LENGTH , IN_CHANNEL), device=device) # Gaussian noise (bsz, seqlen, 768)
    # each prediction involves multiple generation steps
    x_pred = torch.zeros_like(x_t, device=device)
    STEP = 30
    X_SIGMA.to(device)
    X_MEAN.to(device)
    time_start = time.time()
    t = STEP_TOT - 1
    flag = False
    while t > 0:
        t_diff = min(STEP_TOT - 1, t + time_difference)
        if not SELF_COND:
            x_pred = torch.zeros_like(x_t, device=device)
        cond_pred = model(x['image'].to(device), torch.cat([x_t, x_pred], dim=-1).to(device),
                            torch.ones((x["image"].shape[0], MAX_LENGTH), device=device),
                            torch.tensor([t_diff], device=device))

        uncond_pred = model(torch.zeros_like(x["image"], device=device), torch.cat([x_t, x_pred], dim=-1).to(device),
                            torch.ones((x["image"].shape[0], MAX_LENGTH), device=device),
                            # torch.tensor([1, 0], device=device).repeat(x["attention_mask"].shape[0], 1),
                            torch.tensor([t_diff], device=device))
        x_pred = (1 + CLASSIFIER_FREE_WEIGHT) * cond_pred - CLASSIFIER_FREE_WEIGHT * uncond_pred
        # x_pred = cond_pred
        if t < 600 and t > 300 and flag:
            tmp_out = model.lm_head(model.space_decoder(inputs_embeds=x_pred * X_SIGMA + X_MEAN)[0])
            softmax_tmp = nn.functional.softmax(tmp_out, dim=-1)
            # most_confident_token =softmax_tmp.max(dim=-1).values.argmax(dim=-1)
            confidence = softmax_tmp.max(dim=-1).values
            _, idx = torch.sort(confidence, descending=False)
            to_be_updated_idx = idx[:,:MAX_LENGTH//3].to(device)
            gaussian_noise = torch.randn_like(x_pred).to(x_pred.device)
            # x_pred[to_be_updated_idx, :] = gaussian_noise[to_be_updated_idx, :].clone()
            x_t = diffuse_t(x_pred, torch.tensor([t], device=device) - STEP, is_test=True)
            x_t[torch.arange(x_pred.shape[0])[:, None], to_be_updated_idx] = gaussian_noise[torch.arange(x_t.shape[0])[:, None], to_be_updated_idx].clone()
            # indexes1 = nn.functional.softmax(out1, dim=-1).argmax(dim=-1)
            # pred_x0 = (model.space_encoder(indexes1)[0] - X_MEAN)/X_SIGMA
            t = STEP_TOT - 1
            flag = False
        elif t > STEP:
            # noise = pred_x0
            x_t = diffuse_t(x_pred, torch.tensor([t], device=device) - STEP, is_test=True)
            #x_t = p_sample(x_t[:, :MAX_LENGTH, :], x_pred, torch.tensor([t], device=device) , STEP)
        t -= STEP
    cond_pred = x_pred * X_SIGMA + X_MEAN
    out = model.lm_head(model.space_decoder(inputs_embeds=cond_pred)[0])
    indexes = nn.functional.softmax(out, dim=-1).argmax(dim=-1)
    indexes = indexes.unique_consecutive(dim=-1)
    import itertools

    ans_strs = [tokenizer.decode(index) for index in indexes]
    time_end = time.time()
    # print('time cost', time_end - time_start, 's')
    ans_strs = [' '.join(k for k, _ in itertools.groupby(original_str.split())) for original_str in ans_strs]
    ans_strs = [original_str.strip('.').strip() + '.' for original_str in ans_strs]
    ans_strs = [original_str.split('.')[0] + '.' for original_str in ans_strs]
   
    return ans_strs, x['image_id'], x['image_path']


# Load LaDiC Model
def load_model():
    model = Diffuser_with_LN(image_size=224)
    PRETRAINED_DIR = "/home/cvmlserver10/Jin/diffMusic/LaDiC/pretrained_ckpt"
    MODEL_NAME = "/mnt/storage2/Jin/diffMusic/checkpoints/maxlen100_epoch100_newmuimage"
 
    model.visual_encoder, _ = load_checkpoint(model.visual_encoder, f"{PRETRAINED_DIR}/model_base_capfilt_large.pth")
    
    # ëª¨ë¸ ê²½ë¡œë¥¼ í•˜ë“œì½”ë”© ëŒ€ì‹  argparseì—ì„œ ë°›ì•„ì˜¤ë„ë¡ ìˆ˜ì • (ì„ íƒ ì‚¬í•­)
    # model_path = args.model_path 
    model_path = f"{MODEL_NAME}/acc_epoch_15/model.safetensors"
    
    try:
        model.load_state_dict(load_file(model_path), strict=False)
    except FileNotFoundError:
        print(f"Error: Model checkpoint not found at {model_path}")
        sys.exit(1)
        
    return model.to(device)

# Generate music caption using LaDiC and GPT-2 (ì´ í•¨ìˆ˜ëŠ” ìƒˆ ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
def generate_music_caption(image, caption, model, gpt2_model, gpt2_tokenizer, bert_tokenizer, gt_text):
    # ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼, í˜¸ì¶œë˜ì§€ ì•ŠìŒ)
    input_text = f"Generate a detailed music description similar the given reference. Include instruments, rhythm, tempo, and other musical characteristics while preserving the reference's style and meaning. \n\nReference: {caption} \nMusic:"
    generated_text = gpt2_model(input_text, max_new_tokens=50)[0]["generated_text"]
    modified_text = generated_text.split(":")[-1].strip()
    return modified_text

# Generate music using the selected model (ì´ í•¨ìˆ˜ëŠ” ìƒˆ ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
def generate_music(caption, decoder_model, decoder_type, length_in_sec, output_file):
    # ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼, í˜¸ì¶œë˜ì§€ ì•ŠìŒ)
    if decoder_type == "musicgen":
        audio_processor = AutoProcessor.from_pretrained(decoder_model)
        music_generator = MusicgenForConditionalGeneration.from_pretrained(decoder_model).to(device)
        audio_data = audio_processor(text=[caption], padding=True, return_tensors="pt").to(device)
        audio_values = music_generator.generate(**audio_data, max_new_tokens=int(256 * 10 // 5))
        wav_file_data = BytesIO()
        scipy.io.wavfile.write(wav_file_data, rate=16000, data=audio_values[0, 0].cpu().numpy())
    else:
        pipe = AudioLDM2Pipeline.from_pretrained(decoder_model, torch_dtype=torch.float16).to(device)
        audio = pipe(caption, num_inference_steps=50, audio_length_in_s=length_in_sec).audios[0]
        wavfile.write(output_file, rate=16000, data=audio)


# Model evaluation (Image â†’ Caption)
def model_evaluate(model, dataset, dataloader):
    tokenizer = dataset.tokenizer
    model.eval()
    results = []
    image_paths = []
    gt_texts = []

    with torch.no_grad():
        for j, x in tqdm(enumerate(dataloader), desc="Generating MDE text outputs"):
            captions, ids, image_path = inference(x, tokenizer, model, time_difference=5)
 
            image_paths += image_path
            print(x)
            results += x['value']
            gt_texts += x['text'] # Dataloaderê°€ 'text' í‚¤ì— GT ìº¡ì…˜ì„ ë¡œë“œí•œë‹¤ê³  ê°€ì •

    return image_paths, results, gt_texts

# --- 4. Main í•¨ìˆ˜ (ëŒ€í­ ìˆ˜ì •) ---

def main():
    # Load trained model
    model = load_model()

    # Load dataset and DataLoader
    from dataload import create_dataset
    from torch.utils.data import DataLoader

    # (ê²½ë¡œëŠ” í•˜ë“œì½”ë”©ëœ ì›ë³¸ì„ ë”°ë¦„)
    test_csv_file = "/mnt/storage1/Jin/MUImage/MUImageInstructionsEval.json"
    image_dir = "/mnt/storage1/Jin/MUImage/audioset_images_eval"
    audio_dir = "/mnt/storage1/Jin/MUImage/audioset_eval_wav"

    config = {"image_size": 224, "test_ann_file": test_csv_file, "image_root": image_dir, "audio_root": audio_dir}
    
    try:
        test_set = create_dataset("muimage_test", config)
    except ImportError as e:
        print(f"Error: 'dataload' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
        
    test_loader = DataLoader(test_set, shuffle=False, batch_size=64, drop_last=False, num_workers=4)

    # Get generated captions
    print("Running model evaluation to get MDE text outputs...")
    # image_paths: í‰ê°€ëœ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    # generated_captions: MDEê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ (D_gen) ë¦¬ìŠ¤íŠ¸
    # gt_texts: ë°ì´í„°ë¡œë”ê°€ ë¡œë“œí•œ GT í…ìŠ¤íŠ¸ (D_gt) ë¦¬ìŠ¤íŠ¸
    image_paths, generated_captions, gt_texts = model_evaluate(model, test_set, test_loader)
    
    print(f"Successfully generated {len(generated_captions)} text outputs from MDE.")

    # --- [ì‹ ê·œ ìž¥ë¥´ í‰ê°€ ë¡œì§ ì‹œìž‘] ---
    # (ê¸°ì¡´ì˜ GPT-2 ë° ìŒì•… ìƒì„± ì½”ë“œëŠ” ëª¨ë‘ ì‚­ì œ)

    print("\n--- ðŸ“Š Starting Genre Evaluation ---")
    
    # 1. Build Genre Parser
    parent_map, sorted_keywords = build_keyword_map()

    # 2. Loop and Compare
    results = []
    matches = 0
    total_evaluated = 0

    # `generated_captions`ì™€ `gt_texts`ì˜ ê¸¸ì´ê°€ ê°™ë‹¤ê³  ê°€ì •
    if len(generated_captions) != len(gt_texts) or len(generated_captions) != len(image_paths):
        print("Warning: Mismatch in lengths of evaluated data. Zipping to shortest list.")
        
    for i in tqdm(range(len(generated_captions)), desc="Comparing Genres"):
        try:
            image_path = image_paths[i]
            gt_text = gt_texts[i]
            pred_text = generated_captions[i]
        except IndexError:
            continue

        # 3. Get GT Genre
        # (Dataloaderê°€ 'text'ì— ë¬´ì—‡ì„ ë¡œë“œí•˜ëŠ”ì§€ ë¶ˆí™•ì‹¤í•˜ë¯€ë¡œ, 
        #  json íŒŒì¼ì˜ 'conversation[1].caption'ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìžˆìœ¼ë‚˜,
        #  ì¼ë‹¨ì€ dataload.pyê°€ GT ìº¡ì…˜ì„ ë¡œë“œí–ˆë‹¤ê³  ê°€ì •)
        gt_genre = get_parent_genre(gt_text, sorted_keywords, parent_map)

        # Skip samples where GT genre is not clear
        if not gt_genre or gt_genre == 'Other':
            continue

        # 4. Get Predicted Genre
        pred_genre = get_parent_genre(pred_text, sorted_keywords, parent_map)
        if not pred_genre:
            pred_genre = 'Other' # Treat non-genre text as 'Other'

        # 5. Compare
        is_match = (gt_genre == pred_genre)
        if is_match:
            matches += 1
        total_evaluated += 1

        results.append({
            'image_path': image_path,
            'gt_text': gt_text,
            'gt_genre': gt_genre,
            'predicted_text': pred_text,
            'predicted_genre': pred_genre,
            'is_match': is_match
        })

    # 6. Report Final Score
    if total_evaluated > 0:
        accuracy = (matches / total_evaluated) * 100
        print("\n--- ðŸ“Š Evaluation Finished ---")
        print(f"Total Samples Evaluated (with clear GT genre): {total_evaluated}")
        print(f"Correct Genre Matches: {matches}")
        print(f"MDE Genre Accuracy: {accuracy:.2f}%")
    else:
        print("Error: No samples were evaluated. Check GT genre parsing or dataloader.")

    # 7. Save Results
    # `args.output_dir`ì„ ì‚¬ìš©
    output_filename = os.path.join(args.output_dir, "genre_evaluation_results.json")
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump({
            'config': vars(args),
            'metrics': {
                'total_evaluated': total_evaluated,
                'matches': matches,
                'accuracy': accuracy if total_evaluated > 0 else 0
            },
            'results_details': results
        }, f, indent=2)
        
    print(f"Evaluation results saved to {output_filename}")
    
if __name__ == "__main__":
    main()