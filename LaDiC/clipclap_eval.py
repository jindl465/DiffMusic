import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel
import torchaudio.transforms as ta_transforms
from torchvision import transforms
from datasets.melfusion_dataset import MeLFusionDataset
from torch.utils.data import DataLoader
import torch.nn as nn

# âœ… Projection Head ì •ì˜ (Image â†’ ê³µí†µ ê³µê°„)
class ImageProjection(nn.Module):
    def __init__(self, input_dim=512, output_dim=512):
        super().__init__()
        self.projection = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return F.normalize(self.projection(x), dim=-1)

# âœ… Projection Head ì •ì˜ (Audio â†’ ê³µí†µ ê³µê°„)
class AudioProjection(nn.Module):
    def __init__(self, input_dim=512, output_dim=512):
        super().__init__()
        self.projection = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return F.normalize(self.projection(x), dim=-1)

# âœ… Projection Head ì´ˆê¸°í™”
image_projector = ImageProjection().cuda()
audio_projector = AudioProjection().cuda()

# CLIP ëª¨ë¸ ë¡œë“œ
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").cuda()
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# CLAP ëª¨ë¸ ë¡œë“œ (Audio Processing)
clap_model = AutoModel.from_pretrained("laion/larger_clap_music").cuda()
clap_processor = AutoProcessor.from_pretrained("laion/larger_clap_music")

image_transform = transforms.Compose([
    transforms.Resize((224, 224)),  # CLIP ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ì¶¤
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

mel_spectrogram = ta_transforms.MelSpectrogram(
    sample_rate=16000, 
    n_fft=1024, 
    hop_length=512, 
    n_mels=64
).to("cuda")

# âœ… í•™ìŠµëœ Projection Head ë¶ˆëŸ¬ì˜¤ê¸°
checkpoint = torch.load("projection_heads.pth")
image_projector.load_state_dict(checkpoint['image_projector'])
audio_projector.load_state_dict(checkpoint['audio_projector'])
print("âœ… Projection Head ë¡œë“œ ì™„ë£Œ!")

def compute_top_k_accuracy(query_emb, database_embs, ground_truth_idx, k=1):
    """
    Cosine Similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ Top-k Accuracy ì¸¡ì •
    """
    similarities = F.cosine_similarity(query_emb.unsqueeze(0), database_embs)  # (1, N)
    print(ground_truth_idx)
    top_k_indices = torch.argsort(similarities, descending=True)[:k]  # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ
    print(top_k_indices)

    # ì •ë‹µì´ Top-k ë‚´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    return ground_truth_idx in top_k_indices

# âœ… 1. Image â†’ Audio Retrieval Accuracy ì¸¡ì •
def evaluate_image_to_audio(test_dataloader, audio_db, k=1):
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in tqdm(test_dataloader, desc="Evaluating Image â†’ Audio Retrieval"):
            image_input = batch["image"].cuda()
            true_audio_indices = batch["image_id"]  # ì •ë‹µ ì¸ë±ìŠ¤

            image_features = clip_model.get_image_features(image_input)
            image_embeds = image_projector(image_features)

            for i in range(len(image_embeds)):
                is_correct = compute_top_k_accuracy(image_embeds[i], audio_db, true_audio_indices[i], k)
                correct += int(is_correct)

            total += len(image_embeds)

    accuracy = correct / total
    print(f"âœ… Image â†’ Audio Retrieval (Top-{k} Accuracy): {accuracy:.4f}")
    return accuracy

# âœ… 2. Audio â†’ Image Retrieval Accuracy ì¸¡ì •
def evaluate_audio_to_image(test_dataloader, image_db, k=1):
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in tqdm(test_dataloader, desc="Evaluating Audio â†’ Image Retrieval"):
            audio_input = batch["waveform"].cuda()
            true_image_indices = batch["image_id"]  # ì •ë‹µ ì¸ë±ìŠ¤

            # Waveform â†’ Mel-Spectrogram ë³€í™˜
            audio_input = audio_input.mean(dim=1, keepdim=True)  # Mono ë³€í™˜
            mel_input = mel_spectrogram(audio_input).permute(0, 1, 3, 2)  # (batch, 1, 313, 64)

            audio_features = clap_model.get_audio_features(mel_input.cuda())
            audio_embeds = audio_projector(audio_features)

            for i in range(len(audio_embeds)):
                is_correct = compute_top_k_accuracy(audio_embeds[i], image_db, true_image_indices[i], k)
                correct += int(is_correct)

            total += len(audio_embeds)

    accuracy = correct / total
    print(f"âœ… Audio â†’ Image Retrieval (Top-{k} Accuracy): {accuracy:.4f}")
    return accuracy

# âœ… 3. Retrieval DB êµ¬ì¶• (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
def build_test_embedding_database(test_dataloader):
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ & ì˜¤ë””ì˜¤ ì„ë² ë”©ì„ ì €ì¥í•˜ì—¬ í‰ê°€ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
    """
    image_db = []
    audio_db = []
    
    with torch.no_grad():
        for batch in tqdm(test_dataloader, desc="Building Test Retrieval Database"):
            # ì´ë¯¸ì§€ ì„ë² ë”©
            image_input = batch["image"].cuda()
            image_features = clip_model.get_image_features(image_input)
            image_embeds = image_projector(image_features)
            image_db.append(image_embeds)

            # ì˜¤ë””ì˜¤ ì„ë² ë”©
            audio_input = batch["waveform"].cuda()
            audio_input = audio_input.mean(dim=1, keepdim=True)  # Mono ë³€í™˜
            mel_input = mel_spectrogram(audio_input).permute(0, 1, 3, 2)  # (batch, 1, 313, 64)
            audio_features = clap_model.get_audio_features(mel_input.cuda())
            audio_embeds = audio_projector(audio_features)
            audio_db.append(audio_embeds)

    return torch.cat(image_db), torch.cat(audio_db)  # (N, 512), (N, 512)

# MeLFusionDataset ë¡œë“œ
print("start")
image_root = "/mnt/storage1/Jin/melfusion/images"
audio_root = "/mnt/storage1/Jin/melfusion/audios"
ann_file = "/mnt/storage1/Jin/melfusion/test_data.csv"

dataset = MeLFusionDataset(
    transform=image_transform,  # CLIP Processor ì‚¬ìš©
    tokenizer=clap_processor.tokenizer,  # CLAPì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    image_root=image_root,
    ann_file=ann_file,
    audio_root=audio_root
)
test_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)

# âœ… Test Datasetìœ¼ë¡œ Accuracy í‰ê°€
test_image_db, test_audio_db = build_test_embedding_database(test_dataloader)

# Image â†’ Audio Retrieval Accuracy í‰ê°€
top1_acc_img2audio = evaluate_image_to_audio(test_dataloader, test_audio_db, k=1)
top5_acc_img2audio = evaluate_image_to_audio(test_dataloader, test_audio_db, k=5)

# Audio â†’ Image Retrieval Accuracy í‰ê°€
top1_acc_audio2img = evaluate_audio_to_image(test_dataloader, test_image_db, k=1)
top5_acc_audio2img = evaluate_audio_to_image(test_dataloader, test_image_db, k=5)

print(f"ğŸ¯ Final Results:")
print(f"âœ… Image â†’ Audio (Top-1 Accuracy): {top1_acc_img2audio:.4f}")
print(f"âœ… Image â†’ Audio (Top-5 Accuracy): {top5_acc_img2audio:.4f}")
print(f"âœ… Audio â†’ Image (Top-1 Accuracy): {top1_acc_audio2img:.4f}")
print(f"âœ… Audio â†’ Image (Top-5 Accuracy): {top5_acc_audio2img:.4f}")
